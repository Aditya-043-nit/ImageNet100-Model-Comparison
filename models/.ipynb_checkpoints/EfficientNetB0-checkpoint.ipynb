{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d99f4a69-cc40-4ee2-873a-b39f010dfa02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Accuracy\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer\n",
    "from torchvision.models import efficientnet_b0 \n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af9461ba-b297-498c-bab8-bc98e06079bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating mean and std...\n",
      "Mean: tensor([0.4531, 0.4513, 0.3910])\n",
      "Std: tensor([0.1808, 0.1760, 0.1748])\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = (64, 64)  # Setting image size to 64*64, default is 224*224\n",
    "\n",
    "# Define transform to resize and convert to tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "# Replace with your dataset directory\n",
    "dataset_path = r\"PATH_TO_TRAIN_DATA\"\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "\n",
    "# Create the DataLoader\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "# Initialize mean and std\n",
    "mean = 0.0\n",
    "std = 0.0\n",
    "total_images_count = 0\n",
    "\n",
    "# Compute mean and std over the entire dataset\n",
    "print(\"Calculating mean and std...\")\n",
    "for images, _ in loader:\n",
    "    batch_samples = images.size(0)  # batch size (last batch can have smaller size!)\n",
    "    images = images.view(batch_samples, images.size(1), -1)  # reshape to (B, C, H*W)\n",
    "    \n",
    "    mean += images.mean(2).sum(0)\n",
    "    std += images.std(2).sum(0)\n",
    "    total_images_count += batch_samples\n",
    "\n",
    "mean /= total_images_count\n",
    "std /= total_images_count\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Std: {std}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5c0fa63-e405-444c-9b43-99cd1ed19f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for train_dataset\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(64),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "# Define transforms for val_dataset\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3fb994e-01b8-493f-8df7-1e442a930feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(r\"PATH_TO_TRAIN_DATA\",transform= train_transforms)\n",
    "val_dataset = ImageFolder(r\"PATH_TO_VALIDATION_DATA\", transform= val_transforms)\n",
    "\n",
    "train_loader= DataLoader(train_dataset,batch_size=128,shuffle=True,num_workers=4)\n",
    "val_loader= DataLoader(val_dataset,batch_size=128,shuffle=False,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f695a224-01d4-4fab-90fc-e6ebc96d7d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_epoch_metrics_csv(model_name, train_loss_history, val_loss_history,\n",
    "                           train_acc_history, val_top1_acc_history, val_top5_acc_history,\n",
    "                           filename=None):\n",
    "    metrics_dict = {\n",
    "        \"epoch\": list(range(1, len(train_loss_history) + 1)),\n",
    "        \"train_loss\": train_loss_history,\n",
    "        \"val_loss\": val_loss_history,\n",
    "        \"train_acc\": train_acc_history,\n",
    "        \"val_top1\": val_top1_acc_history,\n",
    "        \"val_top5\": val_top5_acc_history,\n",
    "    }\n",
    "    df = pd.DataFrame(metrics_dict)\n",
    "\n",
    "    if filename is None:\n",
    "        filename = f\"results/per_model_logs/{model_name}_metrics.csv\"\n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"[✓] Saved epoch metrics to {filename}\")\n",
    "\n",
    "\n",
    "def append_model_summary(model_name, train_loss_history, val_loss_history,\n",
    "                         train_acc_history, val_top1_acc_history, val_top5_acc_history,\n",
    "                         total_train_time_sec, model, batch_size, optimizer_type,\n",
    "                         lr_schedule_desc, image_size, architecture_type,\n",
    "                         summary_csv_path=\"results/summary/final_model_comparison.csv\"):\n",
    "\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    summary_dict = {\n",
    "        \"model_name\": model_name,\n",
    "        \"final_train_loss\": train_loss_history[-1],\n",
    "        \"final_val_loss\": val_loss_history[-1],\n",
    "        \"final_train_acc\": train_acc_history[-1],\n",
    "        \"final_val_top1_acc\": val_top1_acc_history[-1],\n",
    "        \"final_val_top5_acc\": val_top5_acc_history[-1],\n",
    "        \"total_train_time_sec\": total_train_time_sec,\n",
    "        \"total_train_time_min\": round(total_train_time_sec / 60, 2),\n",
    "        \"num_params\": num_params,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"optimizer\": optimizer_type,\n",
    "        \"learning_rate_schedule\": lr_schedule_desc,\n",
    "        \"image_size\": image_size,\n",
    "        \"architecture_type\": architecture_type,\n",
    "    }\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(summary_csv_path), exist_ok=True)\n",
    "\n",
    "    # Append or create\n",
    "    if os.path.exists(summary_csv_path):\n",
    "        existing_df = pd.read_csv(summary_csv_path)\n",
    "        updated_df = pd.concat([existing_df, pd.DataFrame([summary_dict])], ignore_index=True)\n",
    "    else:\n",
    "        updated_df = pd.DataFrame([summary_dict])\n",
    "\n",
    "    updated_df.to_csv(summary_csv_path, index=False)\n",
    "    print(f\"[✓] Appended model summary to {summary_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc54f9b7-fcfe-4b8f-8511-dd7a6c1aac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing time taken to train the model\n",
    "def print_train_time(start: float,\n",
    "                     end: float,\n",
    "                     device: torch.device = None):\n",
    "  total_time = end-start\n",
    "  print(f\"Train time on {device} : {total_time:.3f} seconds\")\n",
    "  return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b312df95-f1b9-4bca-a225-09348286dad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    loss_fn,\n",
    "    optimizer_adam,\n",
    "    optimizer_sgd,\n",
    "    scheduler_adam,\n",
    "    scheduler_sgd,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    image_size,\n",
    "    architecture_type,\n",
    "    model_name\n",
    "):\n",
    "    # Track metrics\n",
    "    train_loss_history, val_loss_history = [], []\n",
    "    train_acc_history, val_top1_acc_history, val_top5_acc_history = [], [], []\n",
    "\n",
    "    best_top1 = 0.0\n",
    "    best_model_wts = None\n",
    "\n",
    "    start_time = timer()\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\\n{'-'*20}\")\n",
    "\n",
    "        # ----------------\n",
    "        # Select optimizer\n",
    "        # ----------------\n",
    "        if epoch < 30:\n",
    "            optimizer = optimizer_adam\n",
    "            scheduler = scheduler_adam\n",
    "        else:\n",
    "            optimizer = optimizer_sgd\n",
    "            scheduler = scheduler_sgd\n",
    "\n",
    "        # ----------------\n",
    "        # Training Phase\n",
    "        # ----------------\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        total_samples_train = 0\n",
    "\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            batch_size_curr = y.size(0)\n",
    "            total_samples_train += batch_size_curr\n",
    "\n",
    "            outputs = model(X).float()\n",
    "            loss = loss_fn(outputs, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * batch_size_curr\n",
    "            running_correct += (outputs.argmax(dim=1) == y).sum().item()\n",
    "\n",
    "        train_loss = running_loss / total_samples_train\n",
    "        train_acc = (running_correct / total_samples_train) * 100\n",
    "\n",
    "        # ----------------\n",
    "        # Validation Phase\n",
    "        # ----------------\n",
    "        model.eval()\n",
    "        val_loss_total = 0.0\n",
    "        val_samples_total = 0\n",
    "        correct_top1 = 0\n",
    "        correct_top5 = 0\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            for X, y in val_loader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                batch_size_curr = y.size(0)\n",
    "                val_samples_total += batch_size_curr\n",
    "\n",
    "                outputs = model(X).float()\n",
    "                loss = loss_fn(outputs, y)\n",
    "                val_loss_total += loss.item() * batch_size_curr\n",
    "\n",
    "                _, pred_topk = outputs.topk(5, dim=1, largest=True, sorted=True)\n",
    "                correct = pred_topk.eq(y.view(-1, 1).expand_as(pred_topk))\n",
    "\n",
    "                correct_top1 += correct[:, :1].sum().item()\n",
    "                correct_top5 += correct[:, :5].sum().item()\n",
    "\n",
    "        val_loss = val_loss_total / val_samples_total\n",
    "        top1_val_acc = (correct_top1 / val_samples_total) * 100\n",
    "        top5_val_acc = (correct_top5 / val_samples_total) * 100\n",
    "\n",
    "        # ----------------\n",
    "        # Logging\n",
    "        # ----------------\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Top-1 Val Acc: {top1_val_acc:.2f}% | Top-5 Val Acc: {top5_val_acc:.2f}%\")\n",
    "\n",
    "        train_loss_history.append(train_loss)\n",
    "        val_loss_history.append(val_loss)\n",
    "        train_acc_history.append(train_acc)\n",
    "        val_top1_acc_history.append(top1_val_acc)\n",
    "        val_top5_acc_history.append(top5_val_acc)\n",
    "\n",
    "        # Save best model\n",
    "        if top1_val_acc > best_top1:\n",
    "            best_top1 = top1_val_acc\n",
    "            best_model_wts = model.state_dict().copy()\n",
    "            os.makedirs(\"results/checkpoints\", exist_ok=True)\n",
    "            torch.save(best_model_wts, f\"results/checkpoints/{model_name}_best.pth\")\n",
    "            print(f\"[✓] Saved best model (Top-1 = {best_top1:.2f}%)\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    end_time = timer()\n",
    "    total_train_time_sec = end_time - start_time\n",
    "\n",
    "    # Save metrics CSV\n",
    "    save_epoch_metrics_csv(\n",
    "        model_name=model_name,\n",
    "        train_loss_history=train_loss_history,\n",
    "        val_loss_history=val_loss_history,\n",
    "        train_acc_history=train_acc_history,\n",
    "        val_top1_acc_history=val_top1_acc_history,\n",
    "        val_top5_acc_history=val_top5_acc_history\n",
    "    )\n",
    "\n",
    "    # Append summary CSV\n",
    "    append_model_summary(\n",
    "        model_name=model_name,\n",
    "        train_loss_history=train_loss_history,\n",
    "        val_loss_history=val_loss_history,\n",
    "        train_acc_history=train_acc_history,\n",
    "        val_top1_acc_history=val_top1_acc_history,\n",
    "        val_top5_acc_history=val_top5_acc_history,\n",
    "        total_train_time_sec=total_train_time_sec,\n",
    "        model=model,\n",
    "        batch_size=batch_size,\n",
    "        optimizer_type=\"Adam (first 30 epochs) + SGD (rest)\",\n",
    "        lr_schedule_desc=\"StepLR: Adam(step=10, gamma=0.1), SGD(step=30, gamma=0.1)\",\n",
    "        image_size=f\"{image_size[0]}x{image_size[1]}\",\n",
    "        architecture_type=architecture_type\n",
    "    )\n",
    "\n",
    "    print_train_time(start_time, end_time, device)\n",
    "    print(f\"[✓] Training complete for {model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c557f265-8a81-4534-a13b-08aba7967d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (8): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=True)\n",
       "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = efficientnet_b0().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9e16791-7de5-4e2c-bf5c-8d0a136ecceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827eb7548ae044d4aa1da299a18b4a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n",
      "--------------------\n",
      "Train Loss: 4.6433 | Train Acc: 2.84%\n",
      "Val Loss: 4.1711 | Top-1 Val Acc: 5.72% | Top-5 Val Acc: 21.80%\n",
      "[✓] Saved best model (Top-1 = 5.72%)\n",
      "\n",
      "Epoch 2/100\n",
      "--------------------\n",
      "Train Loss: 4.0627 | Train Acc: 7.32%\n",
      "Val Loss: 3.8911 | Top-1 Val Acc: 9.16% | Top-5 Val Acc: 29.22%\n",
      "[✓] Saved best model (Top-1 = 9.16%)\n",
      "\n",
      "Epoch 3/100\n",
      "--------------------\n",
      "Train Loss: 3.8438 | Train Acc: 10.53%\n",
      "Val Loss: 3.6976 | Top-1 Val Acc: 12.54% | Top-5 Val Acc: 36.40%\n",
      "[✓] Saved best model (Top-1 = 12.54%)\n",
      "\n",
      "Epoch 4/100\n",
      "--------------------\n",
      "Train Loss: 3.6579 | Train Acc: 13.58%\n",
      "Val Loss: 3.5171 | Top-1 Val Acc: 15.44% | Top-5 Val Acc: 41.16%\n",
      "[✓] Saved best model (Top-1 = 15.44%)\n",
      "\n",
      "Epoch 5/100\n",
      "--------------------\n",
      "Train Loss: 3.4999 | Train Acc: 16.54%\n",
      "Val Loss: 3.3841 | Top-1 Val Acc: 17.76% | Top-5 Val Acc: 44.98%\n",
      "[✓] Saved best model (Top-1 = 17.76%)\n",
      "\n",
      "Epoch 6/100\n",
      "--------------------\n",
      "Train Loss: 3.3651 | Train Acc: 19.02%\n",
      "Val Loss: 3.2715 | Top-1 Val Acc: 20.28% | Top-5 Val Acc: 48.12%\n",
      "[✓] Saved best model (Top-1 = 20.28%)\n",
      "\n",
      "Epoch 7/100\n",
      "--------------------\n",
      "Train Loss: 3.2554 | Train Acc: 21.11%\n",
      "Val Loss: 3.1627 | Top-1 Val Acc: 22.84% | Top-5 Val Acc: 50.90%\n",
      "[✓] Saved best model (Top-1 = 22.84%)\n",
      "\n",
      "Epoch 8/100\n",
      "--------------------\n",
      "Train Loss: 3.1590 | Train Acc: 22.98%\n",
      "Val Loss: 3.0799 | Top-1 Val Acc: 24.76% | Top-5 Val Acc: 53.48%\n",
      "[✓] Saved best model (Top-1 = 24.76%)\n",
      "\n",
      "Epoch 9/100\n",
      "--------------------\n",
      "Train Loss: 3.0647 | Train Acc: 24.94%\n",
      "Val Loss: 2.9946 | Top-1 Val Acc: 26.10% | Top-5 Val Acc: 55.60%\n",
      "[✓] Saved best model (Top-1 = 26.10%)\n",
      "\n",
      "Epoch 10/100\n",
      "--------------------\n",
      "Train Loss: 3.0003 | Train Acc: 26.29%\n",
      "Val Loss: 2.9500 | Top-1 Val Acc: 26.84% | Top-5 Val Acc: 56.32%\n",
      "[✓] Saved best model (Top-1 = 26.84%)\n",
      "\n",
      "Epoch 11/100\n",
      "--------------------\n",
      "Train Loss: 2.8906 | Train Acc: 28.44%\n",
      "Val Loss: 2.8746 | Top-1 Val Acc: 28.84% | Top-5 Val Acc: 57.88%\n",
      "[✓] Saved best model (Top-1 = 28.84%)\n",
      "\n",
      "Epoch 12/100\n",
      "--------------------\n",
      "Train Loss: 2.8668 | Train Acc: 29.08%\n",
      "Val Loss: 2.8571 | Top-1 Val Acc: 29.42% | Top-5 Val Acc: 58.28%\n",
      "[✓] Saved best model (Top-1 = 29.42%)\n",
      "\n",
      "Epoch 13/100\n",
      "--------------------\n",
      "Train Loss: 2.8569 | Train Acc: 29.38%\n",
      "Val Loss: 2.8453 | Top-1 Val Acc: 29.30% | Top-5 Val Acc: 58.60%\n",
      "\n",
      "Epoch 14/100\n",
      "--------------------\n",
      "Train Loss: 2.8381 | Train Acc: 29.72%\n",
      "Val Loss: 2.8368 | Top-1 Val Acc: 29.78% | Top-5 Val Acc: 58.68%\n",
      "[✓] Saved best model (Top-1 = 29.78%)\n",
      "\n",
      "Epoch 15/100\n",
      "--------------------\n",
      "Train Loss: 2.8276 | Train Acc: 29.80%\n",
      "Val Loss: 2.8302 | Top-1 Val Acc: 29.64% | Top-5 Val Acc: 59.36%\n",
      "\n",
      "Epoch 16/100\n",
      "--------------------\n",
      "Train Loss: 2.8193 | Train Acc: 29.95%\n",
      "Val Loss: 2.8189 | Top-1 Val Acc: 30.20% | Top-5 Val Acc: 59.52%\n",
      "[✓] Saved best model (Top-1 = 30.20%)\n",
      "\n",
      "Epoch 17/100\n",
      "--------------------\n",
      "Train Loss: 2.8039 | Train Acc: 30.43%\n",
      "Val Loss: 2.8162 | Top-1 Val Acc: 30.38% | Top-5 Val Acc: 59.28%\n",
      "[✓] Saved best model (Top-1 = 30.38%)\n",
      "\n",
      "Epoch 18/100\n",
      "--------------------\n",
      "Train Loss: 2.7993 | Train Acc: 30.45%\n",
      "Val Loss: 2.8053 | Top-1 Val Acc: 30.66% | Top-5 Val Acc: 59.88%\n",
      "[✓] Saved best model (Top-1 = 30.66%)\n",
      "\n",
      "Epoch 19/100\n",
      "--------------------\n",
      "Train Loss: 2.7934 | Train Acc: 30.67%\n",
      "Val Loss: 2.7988 | Top-1 Val Acc: 30.56% | Top-5 Val Acc: 59.96%\n",
      "\n",
      "Epoch 20/100\n",
      "--------------------\n",
      "Train Loss: 2.7905 | Train Acc: 30.71%\n",
      "Val Loss: 2.7910 | Top-1 Val Acc: 30.88% | Top-5 Val Acc: 60.32%\n",
      "[✓] Saved best model (Top-1 = 30.88%)\n",
      "\n",
      "Epoch 21/100\n",
      "--------------------\n",
      "Train Loss: 2.7720 | Train Acc: 30.99%\n",
      "Val Loss: 2.7904 | Top-1 Val Acc: 30.94% | Top-5 Val Acc: 60.10%\n",
      "[✓] Saved best model (Top-1 = 30.94%)\n",
      "\n",
      "Epoch 22/100\n",
      "--------------------\n",
      "Train Loss: 2.7731 | Train Acc: 31.12%\n",
      "Val Loss: 2.7889 | Top-1 Val Acc: 30.48% | Top-5 Val Acc: 60.20%\n",
      "\n",
      "Epoch 23/100\n",
      "--------------------\n",
      "Train Loss: 2.7752 | Train Acc: 31.03%\n",
      "Val Loss: 2.7900 | Top-1 Val Acc: 31.04% | Top-5 Val Acc: 60.34%\n",
      "[✓] Saved best model (Top-1 = 31.04%)\n",
      "\n",
      "Epoch 24/100\n",
      "--------------------\n",
      "Train Loss: 2.7678 | Train Acc: 31.13%\n",
      "Val Loss: 2.7875 | Top-1 Val Acc: 31.12% | Top-5 Val Acc: 60.54%\n",
      "[✓] Saved best model (Top-1 = 31.12%)\n",
      "\n",
      "Epoch 25/100\n",
      "--------------------\n",
      "Train Loss: 2.7679 | Train Acc: 31.23%\n",
      "Val Loss: 2.7887 | Top-1 Val Acc: 30.90% | Top-5 Val Acc: 60.42%\n",
      "\n",
      "Epoch 26/100\n",
      "--------------------\n",
      "Train Loss: 2.7685 | Train Acc: 31.21%\n",
      "Val Loss: 2.7819 | Top-1 Val Acc: 31.00% | Top-5 Val Acc: 60.28%\n",
      "\n",
      "Epoch 27/100\n",
      "--------------------\n",
      "Train Loss: 2.7601 | Train Acc: 31.06%\n",
      "Val Loss: 2.7844 | Top-1 Val Acc: 30.94% | Top-5 Val Acc: 60.42%\n",
      "\n",
      "Epoch 28/100\n",
      "--------------------\n",
      "Train Loss: 2.7651 | Train Acc: 31.13%\n",
      "Val Loss: 2.7835 | Top-1 Val Acc: 30.94% | Top-5 Val Acc: 60.40%\n",
      "\n",
      "Epoch 29/100\n",
      "--------------------\n",
      "Train Loss: 2.7707 | Train Acc: 31.23%\n",
      "Val Loss: 2.7847 | Top-1 Val Acc: 30.96% | Top-5 Val Acc: 60.32%\n",
      "\n",
      "Epoch 30/100\n",
      "--------------------\n",
      "Train Loss: 2.7660 | Train Acc: 31.18%\n",
      "Val Loss: 2.7844 | Top-1 Val Acc: 31.00% | Top-5 Val Acc: 60.46%\n",
      "\n",
      "Epoch 31/100\n",
      "--------------------\n",
      "Train Loss: 3.1314 | Train Acc: 23.88%\n",
      "Val Loss: 2.9374 | Top-1 Val Acc: 27.72% | Top-5 Val Acc: 57.20%\n",
      "\n",
      "Epoch 32/100\n",
      "--------------------\n",
      "Train Loss: 2.8519 | Train Acc: 29.14%\n",
      "Val Loss: 2.8058 | Top-1 Val Acc: 30.76% | Top-5 Val Acc: 59.76%\n",
      "\n",
      "Epoch 33/100\n",
      "--------------------\n",
      "Train Loss: 2.7088 | Train Acc: 32.03%\n",
      "Val Loss: 2.7909 | Top-1 Val Acc: 31.64% | Top-5 Val Acc: 61.46%\n",
      "[✓] Saved best model (Top-1 = 31.64%)\n",
      "\n",
      "Epoch 34/100\n",
      "--------------------\n",
      "Train Loss: 2.6034 | Train Acc: 34.60%\n",
      "Val Loss: 2.6231 | Top-1 Val Acc: 33.62% | Top-5 Val Acc: 63.88%\n",
      "[✓] Saved best model (Top-1 = 33.62%)\n",
      "\n",
      "Epoch 35/100\n",
      "--------------------\n",
      "Train Loss: 2.5183 | Train Acc: 36.27%\n",
      "Val Loss: 2.5740 | Top-1 Val Acc: 35.70% | Top-5 Val Acc: 65.18%\n",
      "[✓] Saved best model (Top-1 = 35.70%)\n",
      "\n",
      "Epoch 36/100\n",
      "--------------------\n",
      "Train Loss: 2.4348 | Train Acc: 38.25%\n",
      "Val Loss: 2.4708 | Top-1 Val Acc: 37.64% | Top-5 Val Acc: 68.28%\n",
      "[✓] Saved best model (Top-1 = 37.64%)\n",
      "\n",
      "Epoch 37/100\n",
      "--------------------\n",
      "Train Loss: 2.3701 | Train Acc: 39.57%\n",
      "Val Loss: 2.4602 | Top-1 Val Acc: 38.36% | Top-5 Val Acc: 67.20%\n",
      "[✓] Saved best model (Top-1 = 38.36%)\n",
      "\n",
      "Epoch 38/100\n",
      "--------------------\n",
      "Train Loss: 2.3067 | Train Acc: 41.00%\n",
      "Val Loss: 2.4607 | Top-1 Val Acc: 37.90% | Top-5 Val Acc: 68.14%\n",
      "\n",
      "Epoch 39/100\n",
      "--------------------\n",
      "Train Loss: 2.2716 | Train Acc: 42.03%\n",
      "Val Loss: 2.3366 | Top-1 Val Acc: 39.52% | Top-5 Val Acc: 70.24%\n",
      "[✓] Saved best model (Top-1 = 39.52%)\n",
      "\n",
      "Epoch 40/100\n",
      "--------------------\n",
      "Train Loss: 2.2220 | Train Acc: 42.94%\n",
      "Val Loss: 2.3220 | Top-1 Val Acc: 40.68% | Top-5 Val Acc: 70.56%\n",
      "[✓] Saved best model (Top-1 = 40.68%)\n",
      "\n",
      "Epoch 41/100\n",
      "--------------------\n",
      "Train Loss: 2.1769 | Train Acc: 44.01%\n",
      "Val Loss: 2.2494 | Top-1 Val Acc: 42.06% | Top-5 Val Acc: 71.34%\n",
      "[✓] Saved best model (Top-1 = 42.06%)\n",
      "\n",
      "Epoch 42/100\n",
      "--------------------\n",
      "Train Loss: 2.1527 | Train Acc: 44.60%\n",
      "Val Loss: 2.2572 | Top-1 Val Acc: 42.98% | Top-5 Val Acc: 71.38%\n",
      "[✓] Saved best model (Top-1 = 42.98%)\n",
      "\n",
      "Epoch 43/100\n",
      "--------------------\n",
      "Train Loss: 2.1081 | Train Acc: 45.47%\n",
      "Val Loss: 2.2894 | Top-1 Val Acc: 44.00% | Top-5 Val Acc: 72.80%\n",
      "[✓] Saved best model (Top-1 = 44.00%)\n",
      "\n",
      "Epoch 44/100\n",
      "--------------------\n",
      "Train Loss: 2.0775 | Train Acc: 46.40%\n",
      "Val Loss: 2.1554 | Top-1 Val Acc: 44.56% | Top-5 Val Acc: 73.64%\n",
      "[✓] Saved best model (Top-1 = 44.56%)\n",
      "\n",
      "Epoch 45/100\n",
      "--------------------\n",
      "Train Loss: 2.0492 | Train Acc: 47.05%\n",
      "Val Loss: 2.1543 | Top-1 Val Acc: 43.80% | Top-5 Val Acc: 73.28%\n",
      "\n",
      "Epoch 46/100\n",
      "--------------------\n",
      "Train Loss: 2.0335 | Train Acc: 47.17%\n",
      "Val Loss: 2.1191 | Top-1 Val Acc: 44.24% | Top-5 Val Acc: 73.82%\n",
      "\n",
      "Epoch 47/100\n",
      "--------------------\n",
      "Train Loss: 2.0083 | Train Acc: 47.67%\n",
      "Val Loss: 2.0963 | Top-1 Val Acc: 47.00% | Top-5 Val Acc: 74.22%\n",
      "[✓] Saved best model (Top-1 = 47.00%)\n",
      "\n",
      "Epoch 48/100\n",
      "--------------------\n",
      "Train Loss: 1.9884 | Train Acc: 48.18%\n",
      "Val Loss: 2.1392 | Top-1 Val Acc: 45.44% | Top-5 Val Acc: 73.88%\n",
      "\n",
      "Epoch 49/100\n",
      "--------------------\n",
      "Train Loss: 1.9629 | Train Acc: 48.87%\n",
      "Val Loss: 2.0649 | Top-1 Val Acc: 46.20% | Top-5 Val Acc: 75.58%\n",
      "\n",
      "Epoch 50/100\n",
      "--------------------\n",
      "Train Loss: 1.9496 | Train Acc: 49.17%\n",
      "Val Loss: 2.0821 | Top-1 Val Acc: 46.00% | Top-5 Val Acc: 74.72%\n",
      "\n",
      "Epoch 51/100\n",
      "--------------------\n",
      "Train Loss: 1.9342 | Train Acc: 49.62%\n",
      "Val Loss: 2.1084 | Top-1 Val Acc: 45.96% | Top-5 Val Acc: 74.56%\n",
      "\n",
      "Epoch 52/100\n",
      "--------------------\n",
      "Train Loss: 1.9189 | Train Acc: 49.90%\n",
      "Val Loss: 2.0902 | Top-1 Val Acc: 45.34% | Top-5 Val Acc: 74.14%\n",
      "\n",
      "Epoch 53/100\n",
      "--------------------\n",
      "Train Loss: 1.9082 | Train Acc: 50.31%\n",
      "Val Loss: 2.0165 | Top-1 Val Acc: 46.92% | Top-5 Val Acc: 75.96%\n",
      "\n",
      "Epoch 54/100\n",
      "--------------------\n",
      "Train Loss: 1.8921 | Train Acc: 50.52%\n",
      "Val Loss: 2.0066 | Top-1 Val Acc: 48.52% | Top-5 Val Acc: 75.94%\n",
      "[✓] Saved best model (Top-1 = 48.52%)\n",
      "\n",
      "Epoch 55/100\n",
      "--------------------\n",
      "Train Loss: 1.8851 | Train Acc: 50.80%\n",
      "Val Loss: 2.0181 | Top-1 Val Acc: 48.12% | Top-5 Val Acc: 76.42%\n",
      "\n",
      "Epoch 56/100\n",
      "--------------------\n",
      "Train Loss: 1.8656 | Train Acc: 51.25%\n",
      "Val Loss: 2.0035 | Top-1 Val Acc: 47.96% | Top-5 Val Acc: 76.48%\n",
      "\n",
      "Epoch 57/100\n",
      "--------------------\n",
      "Train Loss: 1.8634 | Train Acc: 51.28%\n",
      "Val Loss: 1.9885 | Top-1 Val Acc: 48.04% | Top-5 Val Acc: 76.56%\n",
      "\n",
      "Epoch 58/100\n",
      "--------------------\n",
      "Train Loss: 1.8479 | Train Acc: 51.65%\n",
      "Val Loss: 1.9742 | Top-1 Val Acc: 48.80% | Top-5 Val Acc: 76.90%\n",
      "[✓] Saved best model (Top-1 = 48.80%)\n",
      "\n",
      "Epoch 59/100\n",
      "--------------------\n",
      "Train Loss: 1.8397 | Train Acc: 51.98%\n",
      "Val Loss: 1.9458 | Top-1 Val Acc: 49.56% | Top-5 Val Acc: 77.64%\n",
      "[✓] Saved best model (Top-1 = 49.56%)\n",
      "\n",
      "Epoch 60/100\n",
      "--------------------\n",
      "Train Loss: 1.8353 | Train Acc: 51.94%\n",
      "Val Loss: 1.9579 | Top-1 Val Acc: 48.68% | Top-5 Val Acc: 77.10%\n",
      "\n",
      "Epoch 61/100\n",
      "--------------------\n",
      "Train Loss: 1.5863 | Train Acc: 58.10%\n",
      "Val Loss: 1.6593 | Top-1 Val Acc: 56.36% | Top-5 Val Acc: 81.90%\n",
      "[✓] Saved best model (Top-1 = 56.36%)\n",
      "\n",
      "Epoch 62/100\n",
      "--------------------\n",
      "Train Loss: 1.5222 | Train Acc: 59.50%\n",
      "Val Loss: 1.6720 | Top-1 Val Acc: 56.06% | Top-5 Val Acc: 82.04%\n",
      "\n",
      "Epoch 63/100\n",
      "--------------------\n",
      "Train Loss: 1.5004 | Train Acc: 60.08%\n",
      "Val Loss: 1.6433 | Top-1 Val Acc: 56.68% | Top-5 Val Acc: 82.08%\n",
      "[✓] Saved best model (Top-1 = 56.68%)\n",
      "\n",
      "Epoch 64/100\n",
      "--------------------\n",
      "Train Loss: 1.4864 | Train Acc: 60.39%\n",
      "Val Loss: 1.6507 | Top-1 Val Acc: 56.84% | Top-5 Val Acc: 82.54%\n",
      "[✓] Saved best model (Top-1 = 56.84%)\n",
      "\n",
      "Epoch 65/100\n",
      "--------------------\n",
      "Train Loss: 1.4696 | Train Acc: 60.73%\n",
      "Val Loss: 1.5987 | Top-1 Val Acc: 57.62% | Top-5 Val Acc: 82.70%\n",
      "[✓] Saved best model (Top-1 = 57.62%)\n",
      "\n",
      "Epoch 66/100\n",
      "--------------------\n",
      "Train Loss: 1.4618 | Train Acc: 60.96%\n",
      "Val Loss: 1.6095 | Top-1 Val Acc: 57.22% | Top-5 Val Acc: 82.92%\n",
      "\n",
      "Epoch 67/100\n",
      "--------------------\n",
      "Train Loss: 1.4491 | Train Acc: 61.32%\n",
      "Val Loss: 1.6131 | Top-1 Val Acc: 57.22% | Top-5 Val Acc: 82.72%\n",
      "\n",
      "Epoch 68/100\n",
      "--------------------\n",
      "Train Loss: 1.4466 | Train Acc: 61.24%\n",
      "Val Loss: 1.5956 | Top-1 Val Acc: 57.76% | Top-5 Val Acc: 83.18%\n",
      "[✓] Saved best model (Top-1 = 57.76%)\n",
      "\n",
      "Epoch 69/100\n",
      "--------------------\n",
      "Train Loss: 1.4369 | Train Acc: 61.54%\n",
      "Val Loss: 1.5907 | Top-1 Val Acc: 57.62% | Top-5 Val Acc: 83.54%\n",
      "\n",
      "Epoch 70/100\n",
      "--------------------\n",
      "Train Loss: 1.4299 | Train Acc: 61.56%\n",
      "Val Loss: 1.5882 | Top-1 Val Acc: 57.64% | Top-5 Val Acc: 83.56%\n",
      "\n",
      "Epoch 71/100\n",
      "--------------------\n",
      "Train Loss: 1.4282 | Train Acc: 61.72%\n",
      "Val Loss: 1.6003 | Top-1 Val Acc: 57.70% | Top-5 Val Acc: 83.14%\n",
      "\n",
      "Epoch 72/100\n",
      "--------------------\n",
      "Train Loss: 1.4253 | Train Acc: 61.80%\n",
      "Val Loss: 1.5662 | Top-1 Val Acc: 58.64% | Top-5 Val Acc: 83.48%\n",
      "[✓] Saved best model (Top-1 = 58.64%)\n",
      "\n",
      "Epoch 73/100\n",
      "--------------------\n",
      "Train Loss: 1.4145 | Train Acc: 62.09%\n",
      "Val Loss: 1.5798 | Top-1 Val Acc: 58.36% | Top-5 Val Acc: 83.34%\n",
      "\n",
      "Epoch 74/100\n",
      "--------------------\n",
      "Train Loss: 1.4097 | Train Acc: 62.06%\n",
      "Val Loss: 1.5729 | Top-1 Val Acc: 58.08% | Top-5 Val Acc: 83.22%\n",
      "\n",
      "Epoch 75/100\n",
      "--------------------\n",
      "Train Loss: 1.4087 | Train Acc: 62.31%\n",
      "Val Loss: 1.5601 | Top-1 Val Acc: 58.78% | Top-5 Val Acc: 83.44%\n",
      "[✓] Saved best model (Top-1 = 58.78%)\n",
      "\n",
      "Epoch 76/100\n",
      "--------------------\n",
      "Train Loss: 1.4047 | Train Acc: 62.36%\n",
      "Val Loss: 1.5814 | Top-1 Val Acc: 58.06% | Top-5 Val Acc: 83.32%\n",
      "\n",
      "Epoch 77/100\n",
      "--------------------\n",
      "Train Loss: 1.3984 | Train Acc: 62.24%\n",
      "Val Loss: 1.5659 | Top-1 Val Acc: 58.30% | Top-5 Val Acc: 83.68%\n",
      "\n",
      "Epoch 78/100\n",
      "--------------------\n",
      "Train Loss: 1.3908 | Train Acc: 62.53%\n",
      "Val Loss: 1.5752 | Top-1 Val Acc: 58.46% | Top-5 Val Acc: 83.76%\n",
      "\n",
      "Epoch 79/100\n",
      "--------------------\n",
      "Train Loss: 1.3894 | Train Acc: 62.70%\n",
      "Val Loss: 1.5673 | Top-1 Val Acc: 58.50% | Top-5 Val Acc: 83.70%\n",
      "\n",
      "Epoch 80/100\n",
      "--------------------\n",
      "Train Loss: 1.3860 | Train Acc: 62.70%\n",
      "Val Loss: 1.5674 | Top-1 Val Acc: 57.96% | Top-5 Val Acc: 83.42%\n",
      "\n",
      "Epoch 81/100\n",
      "--------------------\n",
      "Train Loss: 1.3874 | Train Acc: 62.65%\n",
      "Val Loss: 1.5727 | Top-1 Val Acc: 58.54% | Top-5 Val Acc: 83.26%\n",
      "\n",
      "Epoch 82/100\n",
      "--------------------\n",
      "Train Loss: 1.3801 | Train Acc: 62.85%\n",
      "Val Loss: 1.5609 | Top-1 Val Acc: 59.16% | Top-5 Val Acc: 83.50%\n",
      "[✓] Saved best model (Top-1 = 59.16%)\n",
      "\n",
      "Epoch 83/100\n",
      "--------------------\n",
      "Train Loss: 1.3715 | Train Acc: 63.11%\n",
      "Val Loss: 1.5393 | Top-1 Val Acc: 59.04% | Top-5 Val Acc: 83.90%\n",
      "\n",
      "Epoch 84/100\n",
      "--------------------\n",
      "Train Loss: 1.3724 | Train Acc: 62.99%\n",
      "Val Loss: 1.5726 | Top-1 Val Acc: 58.64% | Top-5 Val Acc: 83.20%\n",
      "\n",
      "Epoch 85/100\n",
      "--------------------\n",
      "Train Loss: 1.3718 | Train Acc: 63.01%\n",
      "Val Loss: 1.5508 | Top-1 Val Acc: 58.54% | Top-5 Val Acc: 83.76%\n",
      "\n",
      "Epoch 86/100\n",
      "--------------------\n",
      "Train Loss: 1.3645 | Train Acc: 63.33%\n",
      "Val Loss: 1.5507 | Top-1 Val Acc: 59.26% | Top-5 Val Acc: 83.96%\n",
      "[✓] Saved best model (Top-1 = 59.26%)\n",
      "\n",
      "Epoch 87/100\n",
      "--------------------\n",
      "Train Loss: 1.3628 | Train Acc: 63.32%\n",
      "Val Loss: 1.5479 | Top-1 Val Acc: 59.22% | Top-5 Val Acc: 83.60%\n",
      "\n",
      "Epoch 88/100\n",
      "--------------------\n",
      "Train Loss: 1.3599 | Train Acc: 63.36%\n",
      "Val Loss: 1.5641 | Top-1 Val Acc: 58.64% | Top-5 Val Acc: 83.68%\n",
      "\n",
      "Epoch 89/100\n",
      "--------------------\n",
      "Train Loss: 1.3593 | Train Acc: 63.39%\n",
      "Val Loss: 1.5494 | Top-1 Val Acc: 58.82% | Top-5 Val Acc: 84.06%\n",
      "\n",
      "Epoch 90/100\n",
      "--------------------\n",
      "Train Loss: 1.3581 | Train Acc: 63.40%\n",
      "Val Loss: 1.5478 | Top-1 Val Acc: 58.86% | Top-5 Val Acc: 84.18%\n",
      "\n",
      "Epoch 91/100\n",
      "--------------------\n",
      "Train Loss: 1.3104 | Train Acc: 64.61%\n",
      "Val Loss: 1.5026 | Top-1 Val Acc: 60.56% | Top-5 Val Acc: 84.66%\n",
      "[✓] Saved best model (Top-1 = 60.56%)\n",
      "\n",
      "Epoch 92/100\n",
      "--------------------\n",
      "Train Loss: 1.2833 | Train Acc: 65.30%\n",
      "Val Loss: 1.5008 | Top-1 Val Acc: 60.48% | Top-5 Val Acc: 84.72%\n",
      "\n",
      "Epoch 93/100\n",
      "--------------------\n",
      "Train Loss: 1.2808 | Train Acc: 65.30%\n",
      "Val Loss: 1.4940 | Top-1 Val Acc: 60.70% | Top-5 Val Acc: 84.88%\n",
      "[✓] Saved best model (Top-1 = 60.70%)\n",
      "\n",
      "Epoch 94/100\n",
      "--------------------\n",
      "Train Loss: 1.2778 | Train Acc: 65.47%\n",
      "Val Loss: 1.5029 | Top-1 Val Acc: 60.70% | Top-5 Val Acc: 84.64%\n",
      "\n",
      "Epoch 95/100\n",
      "--------------------\n",
      "Train Loss: 1.2783 | Train Acc: 65.43%\n",
      "Val Loss: 1.4984 | Top-1 Val Acc: 60.64% | Top-5 Val Acc: 84.56%\n",
      "\n",
      "Epoch 96/100\n",
      "--------------------\n",
      "Train Loss: 1.2748 | Train Acc: 65.51%\n",
      "Val Loss: 1.4926 | Top-1 Val Acc: 60.82% | Top-5 Val Acc: 84.70%\n",
      "[✓] Saved best model (Top-1 = 60.82%)\n",
      "\n",
      "Epoch 97/100\n",
      "--------------------\n",
      "Train Loss: 1.2716 | Train Acc: 65.63%\n",
      "Val Loss: 1.4954 | Top-1 Val Acc: 60.44% | Top-5 Val Acc: 84.76%\n",
      "\n",
      "Epoch 98/100\n",
      "--------------------\n",
      "Train Loss: 1.2710 | Train Acc: 65.56%\n",
      "Val Loss: 1.4890 | Top-1 Val Acc: 61.18% | Top-5 Val Acc: 84.74%\n",
      "[✓] Saved best model (Top-1 = 61.18%)\n",
      "\n",
      "Epoch 99/100\n",
      "--------------------\n",
      "Train Loss: 1.2673 | Train Acc: 65.54%\n",
      "Val Loss: 1.5031 | Top-1 Val Acc: 60.28% | Top-5 Val Acc: 84.66%\n",
      "\n",
      "Epoch 100/100\n",
      "--------------------\n",
      "Train Loss: 1.2730 | Train Acc: 65.57%\n",
      "Val Loss: 1.4867 | Top-1 Val Acc: 60.64% | Top-5 Val Acc: 84.76%\n",
      "[✓] Saved epoch metrics to results/per_model_logs/EfficientB0_ImageNet100_metrics.csv\n",
      "[✓] Appended model summary to results/summary/final_model_comparison.csv\n",
      "Train time on cuda : 15665.489 seconds\n",
      "[✓] Training complete for EfficientB0_ImageNet100\n"
     ]
    }
   ],
   "source": [
    "# Defining loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Defining optimizers\n",
    "optimizer_adam = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "optimizer_sgd = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Using learning rate schedulers \n",
    "scheduler_adam = torch.optim.lr_scheduler.StepLR(optimizer_adam, step_size=10, gamma=0.1)\n",
    "scheduler_sgd = torch.optim.lr_scheduler.StepLR(optimizer_sgd, step_size=30, gamma=0.1)\n",
    "\n",
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer_adam=optimizer_adam,\n",
    "    optimizer_sgd=optimizer_sgd,\n",
    "    scheduler_adam=scheduler_adam,\n",
    "    scheduler_sgd=scheduler_sgd,\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    image_size=(64, 64),\n",
    "    architecture_type=\"EfficientB0\",\n",
    "    model_name=\"EfficientB0_ImageNet100\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fab3bd3-2020-4d93-9551-e135d87fa10f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
